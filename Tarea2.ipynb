{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREGUNTA 1.\n",
    "\n",
    "Primero, leeré los datos para ver su estructura y determinar qué columnas se utilizarán como características (X) y cuál será la variable objetivo (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\loqbr\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\loqbr\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\loqbr\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\loqbr\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\loqbr\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\loqbr\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "   Position    Artist Name                                   Song Name  Days  \\\n",
      "0         1   Post Malone   Sunflower  SpiderMan: Into the SpiderVerse  1506   \n",
      "1         2    Juice WRLD                                 Lucid Dreams  1673   \n",
      "2         3  Lil Uzi Vert                                XO TOUR Llif3  1853   \n",
      "3         4       J. Cole                               No Role Modelz  2547   \n",
      "4         5   Post Malone                                     rockstar  1223   \n",
      "\n",
      "   Top 10 (xTimes)  Peak Position Peak Position (xTimes)  Peak Streams  \\\n",
      "0            302.0              1                  (x29)       2118242   \n",
      "1            178.0              1                  (x20)       2127668   \n",
      "2            212.0              1                   (x4)       1660502   \n",
      "3              6.0              7                      0        659366   \n",
      "4            186.0              1                 (x124)       2905678   \n",
      "\n",
      "   Total Streams  \n",
      "0      883369738  \n",
      "1      864832399  \n",
      "2      781153024  \n",
      "3      734857487  \n",
      "4      718865961  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11084 entries, 0 to 11083\n",
      "Data columns (total 9 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Position                11084 non-null  int64  \n",
      " 1   Artist Name             11084 non-null  object \n",
      " 2   Song Name               11080 non-null  object \n",
      " 3   Days                    11084 non-null  int64  \n",
      " 4   Top 10 (xTimes)         11084 non-null  float64\n",
      " 5   Peak Position           11084 non-null  int64  \n",
      " 6   Peak Position (xTimes)  11084 non-null  object \n",
      " 7   Peak Streams            11084 non-null  int64  \n",
      " 8   Total Streams           11084 non-null  int64  \n",
      "dtypes: float64(1), int64(5), object(3)\n",
      "memory usage: 779.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"./Spotify_final_dataset.csv\")\n",
    "cabezeras = dataset.head()\n",
    "print(cabezeras)\n",
    "\n",
    "print(dataset.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este tipo de dataset, se puede utilizar la herramienta de regresion logistica, con la que se puede utilizar para clasificar los datos en si han estado o no en el top 10, basandose en las caracteristicas del dato, como lo puede ser peak position. Por lo que tendremos solamente dos valores a predecir, esta herramienta será ideal.\n",
    "\n",
    "Para poder lograr escalar los datos de manera eficiente, se va a utilizar un preprocesamiento de los datos con StandarScaler, el cual nos va a permitir caracterizar los datos a una escala con media 0 y desviación estandar 1. Esto va a facilitar la convergencia de los datos.\n",
    "\n",
    "Luego para realizar el entrenamiento de la herramienta Regresion logistica, definimos que el 70% de los datos serán aplicados para el entrenamiento y el 30% para probar el rendimiento de este. De esta forma, se utilizarán datos ditintos a los utilizados para el entrenamiento de este. Además, como se puede ver en la configuracion de la herramienta, se le van a dar dos parametros, donde max_iter va a corresponder al aumento de iteraciones para asegurar que tenga tiempo de converger, y solver = 'lbfgs' donde este parametro va a optimizar la variables con un metodo llamado Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS), el cual va a mejorar la eficiencia, usando la matriz Hessiana para encontrar el mínimo de una función, pero solo una pequeña informacion (ya que limita la memoria). Luego de realizar estas configuraciones previas, se ejecuta el entrenamiento con la funcion fit(), dandole los parametros de entrenamiento.\n",
    "\n",
    "Luego de esto ya se procede a evaluar el rendimiento de la herramienta ya entrenada dandole los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Position', 'Artist Name', 'Song Name', 'Days', 'Top 10 (xTimes)',\n",
      "       'Peak Position', 'Peak Position (xTimes)', 'Peak Streams',\n",
      "       'Total Streams'],\n",
      "      dtype='object')\n",
      "Accuracy: 0.81\n",
      "F1-Score: 0.84\n",
      "Matriz de Confusión:\n",
      "[[2615  197   19 ...    0    0    0]\n",
      " [   0   58    7 ...    0    0    0]\n",
      " [   0   23    4 ...    0    0    0]\n",
      " ...\n",
      " [   0    0    0 ...    0    0    0]\n",
      " [   0    0    0 ...    1    0    0]\n",
      " [   0    0    0 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "file_path = \"./Spotify_final_dataset.csv\"\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "print(dataset.columns)\n",
    "\n",
    "dataCleaned = dataset.drop(columns=['Position', 'Artist Name', 'Song Name'])\n",
    "\n",
    "label_encoders = {}\n",
    "for col in dataCleaned.select_dtypes(include=['object']).columns:\n",
    "    labelEncoder = LabelEncoder()\n",
    "    dataCleaned[col] = labelEncoder.fit_transform(dataCleaned[col])\n",
    "    label_encoders[col] = labelEncoder\n",
    "\n",
    "X = dataCleaned.drop(columns=['Top 10 (xTimes)'])\n",
    "y = dataCleaned['Top 10 (xTimes)']\n",
    "\n",
    "scalarDatos = StandardScaler()\n",
    "X_escalados = scalarDatos.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_escalados, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, solver='lbfgs', class_weight='balanced') \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, se calculan las metricas accuracy y f1 score, donde accuracy corresponde a la proporción de predicciones correctas respecto al total de observaciones, y f1 score corresponde a el cálculo de la media aritmética ponderada de la precisión y la sensibilidad.\n",
    "\n",
    "Al ser un dataset con tantos datos, se logra optimizar con éxito el dataset, logrando hacer más eficiente esta herramienta. Como se ve, se obtiene un f1-score de 0.84, lo que indica que la presición del modelo  es muy eficiente. Además se logra obener un Accuracy de 0.81."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREGUNTA 2.\n",
    "\n",
    "A continuación se aplica el algoritmo de KNN, el cual consiste en aplicar el valor de K que va a definir el numero de vecinos que se concideran para realizar la clasificación de los datos. A continuación se realiza el algoritmo KNeighborsClassifier en python. Se tiene en cuenta el mismo procesamiento realizado anteriormente al dataset para optimizar y mantener la eficiencia del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 3 - Accuracy: 0.90 - F1-Score: 0.89\n",
      "K: 5 - Accuracy: 0.90 - F1-Score: 0.89\n",
      "K: 7 - Accuracy: 0.90 - F1-Score: 0.89\n",
      "K: 9 - Accuracy: 0.90 - F1-Score: 0.88\n",
      "K: 11 - Accuracy: 0.90 - F1-Score: 0.88\n",
      "K: 15 - Accuracy: 0.90 - F1-Score: 0.88\n",
      "K: 18 - Accuracy: 0.90 - F1-Score: 0.88\n",
      "K: 20 - Accuracy: 0.90 - F1-Score: 0.88\n",
      "K: 50 - Accuracy: 0.89 - F1-Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "path = './Spotify_final_dataset.csv'\n",
    "dataset = pd.read_csv(path)\n",
    "\n",
    "dataCleaned = dataset.drop(columns = ['Position', 'Artist Name', 'Song Name'])\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "label_encoders = {}\n",
    "for col in dataCleaned.select_dtypes(include=['object']).columns:\n",
    "    labelEncoder = LabelEncoder()\n",
    "    dataCleaned[col] = labelEncoder.fit_transform(dataCleaned[col])\n",
    "    label_encoders[col] = labelEncoder\n",
    "\n",
    "X = dataCleaned.drop(columns=['Top 10 (xTimes)'])  \n",
    "y = dataCleaned['Top 10 (xTimes)']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_escalado = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_escalado, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "k_value = [3, 5, 7, 9, 11, 15, 18, 20, 50]\n",
    "\n",
    "for k in k_value:\n",
    "    model = KNeighborsClassifier(n_neighbors = k)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "    print(f\"K: {k} - Accuracy: {accuracy:.2f} - F1-Score: {f1:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, los resultados son bastante similares con respecto a la regresion logistica. Sin embargo, KNN posee una precisión mejor que en regresión logistica. Por lo que para este tipo de problema, o para la clasificación de estos datos, es mas adecuado aplicar KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREGUNTA 3\n",
    "\n",
    "A continuación se procede a utilizar la herramienta DBSCAN y GMM. Donde se realizan algunos procesamiento en los datos para la optimización de las herramientas, ya que se esta trabajando con grandes volúmenes de datos. Esto se utiliza para poder realizar una comparación entre ambos algoritmos. Se van a utilizar las mismas etiquetas definidas en las preguntas 1 y 2, por lo que se podra notar diferencias en el funcionamiento de los algoritmos de clasificación.\n",
    "\n",
    "DBSCAN es un algoritmo de clustering, donde se va a definir un eps, que va a corresponder a un radio de la vecindad que se debe conciderar para generar el cluster. Luego GMM es un modelo probabilistico que toma los datos como una mezcla de distribuciones gaussianas. Este va a tomar un promedio y una desviación estandar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados DBSCAN:\n",
      "Configuracion (eps=0.3, min_samples=5): Silhouette Score = 0.33\n",
      "Configuracion (eps=0.5, min_samples=5): Silhouette Score = 0.55\n",
      "Configuracion (eps=0.3, min_samples=10): Silhouette Score = 0.67\n",
      "Configuracion (eps=0.5, min_samples=10): Silhouette Score = 0.76\n",
      "\n",
      "GMM Silhouette Score: -0.25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "file_path = './Spotify_final_dataset.csv'\n",
    "dataset = pd.read_csv(file_path)\n",
    "dataCleaned = dataset.drop(columns=['Position', 'Artist Name', 'Song Name'])\n",
    "\n",
    "label_encoders = {}\n",
    "for col in dataCleaned.select_dtypes(include=['object']).columns:\n",
    "    labelEncoder = LabelEncoder()\n",
    "    dataCleaned[col] = labelEncoder.fit_transform(dataCleaned[col])\n",
    "    label_encoders[col] = labelEncoder\n",
    "\n",
    "# Separar características\n",
    "X = dataCleaned.drop(columns=['Top 10 (xTimes)'])  # Ajustar según la columna objetivo de tu CSV\n",
    "X_escalado = StandardScaler().fit_transform(X)\n",
    "\n",
    "dbscan_conf = [\n",
    "    {'eps': 0.3, 'min_samples': 5},\n",
    "    {'eps': 0.5, 'min_samples': 5},\n",
    "    {'eps': 0.3, 'min_samples': 10},\n",
    "    {'eps': 0.5, 'min_samples': 10}\n",
    "]\n",
    "\n",
    "\n",
    "dbscan_results = {}\n",
    "for conf in dbscan_conf:\n",
    "    dbscan = DBSCAN(eps=conf['eps'], min_samples=conf['min_samples'])\n",
    "    labels = dbscan.fit_predict(X_escalado)\n",
    "    if len(set(labels)) > 1:\n",
    "        silhouette = silhouette_score(X_escalado[labels != -1], labels[labels != -1])\n",
    "    else:\n",
    "        silhouette = -1\n",
    "    dbscan_results[(conf['eps'], conf['min_samples'])] = silhouette\n",
    "\n",
    "n_comps = len(dataCleaned['Top 10 (xTimes)'].unique())\n",
    "gmm = GaussianMixture(n_components=n_comps, random_state=42)\n",
    "gmm_labels = gmm.fit_predict(X_escalado)\n",
    "gmm_silhouette = silhouette_score(X_escalado, gmm_labels)\n",
    "\n",
    "\n",
    "print(\"Resultados DBSCAN:\")\n",
    "for conf, score in dbscan_results.items():\n",
    "    print(f\"Configuracion (eps={conf[0]}, min_samples={conf[1]}): Silhouette Score = {score:.2f}\")\n",
    "\n",
    "print(f\"\\nGMM Silhouette Score: {gmm_silhouette:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder realizar una comparación de los datos, en DBSCAN se utilizan distintos parametros, y se obtiene el Silhouette score, que se utiliza para evaluar la calidad de cada clustering. De esta manera podremos tener un parámetro para poder medir cada resultado y comparar ambos algoritmos. \n",
    "\n",
    "Como se puede observar, en DBSCAN va a variar en cada uno de los eps definidos, donde en al dar valores mas pequeños se va a obtener un Silhouette Score mas bajo, lo que indica que se formarán clusters mas pequeños y menos definidos (distintos entre sí). Al aumentar estos parametros de eps y min_samples, se obtendra un silhouette Score mas alto, lo que muestra que se formarán clusters mas grandes y definidos. Sin embargo, en GMM se muestra un Silhouette Score de de -0.25, que al dar un número negativo, va a mostrar que estos clusters están mal definidos.\n",
    "\n",
    "Por lo tanto, GMM será mas eficiente para datos que se adapten más a distribuciones Gaussianas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
